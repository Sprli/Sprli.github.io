<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>cmake学习记录</title>
      <link href="/2023/05/26/Cmake_Study/"/>
      <url>/2023/05/26/Cmake_Study/</url>
      
        <content type="html"><![CDATA[<h2 id="问题背景">问题背景</h2><p>这个问题是我在做海赛C2比赛编写部署自动打靶算法遇到的。先简单叙述一下起因，由于我们C2队伍使用的是Jetson Nano来识别靶心，进而实现自动打靶功能，于是算法识别这方面就由我来进展。在网上参考了一些部署方案。最终选择了使用Deepstream架构运行yolov5n的trt模型进行识别检测。但是由于我们的需求不仅是检测到靶心，还要将检测到的靶心的位置传递给下位机stm32，因此我需要在该架构中加入串口通信模块，实现该功能。由于Deepstream在国内的相关资料很少，故需要自己去了解并实现添加该模块到这个框架中。而且其代码基本都是C++代码，过程中要用到cmake等编译工具，故而接触到了cmake相关文件的编写与使用，并写下该篇博客对其进行记录。</p><h2 id="一、cmake是什么">一、cmake是什么</h2><p>CMake是一个跨平台的编译(Build)工具,可以用简单的语句来描述所有平台的编译过程。<br>CMake能够输出各种各样的makefile或者project文件，能测试编译器所支持的C++特性。<br>假如我们有一个深度学习框架的部分工程列表，里面有超过40个互相调用的工程共同组成，一些用于生成库文件，一些用于实现逻辑功能。他们之间的调用关系复杂而严格，如果我想在这样复杂的框架下进行二次开发，显然只拥有它的源码是远远不够的，还需要清楚的明白这几十个项目之间的复杂关系，在没有原作者的帮助下进行这项工作几乎是不可能的。即使原作者给出了相关的结构文档，对新手来说建立工程的过程依旧是漫长而艰辛的，因此CMake的作用就凸显出来了。原作者只需要生成一份CMakeLists.txt文档，框架的使用者们只需要在下载源码的同时下载作者提供的CMakeLists.txt，就可以利用CMake，在“原作者的帮助下”进行工程的搭建。</p><h2 id="CMakeLists-txt的语法">CMakeLists.txt的语法</h2><p>先附上相关代码</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cmake_minimum_required<span class="token punctuation">(</span>VERSION <span class="token number">3.10</span><span class="token punctuation">)</span>project<span class="token punctuation">(</span>yolov5<span class="token punctuation">)</span>add_definitions<span class="token punctuation">(</span>-std<span class="token operator">=</span>c++11<span class="token punctuation">)</span>add_definitions<span class="token punctuation">(</span>-DAPI_EXPORTS<span class="token punctuation">)</span>option<span class="token punctuation">(</span>CUDA_USE_STATIC_CUDA_RUNTIME OFF<span class="token punctuation">)</span>set<span class="token punctuation">(</span>CMAKE_CXX_STANDARD <span class="token number">11</span><span class="token punctuation">)</span>set<span class="token punctuation">(</span>CMAKE_BUILD_TYPE Debug<span class="token punctuation">)</span><span class="token comment"># TODO(Call for PR): make cmake compatible with Windows</span>set<span class="token punctuation">(</span>CMAKE_CUDA_COMPILER /usr/local/cuda/bin/nvcc<span class="token punctuation">)</span>enable_language<span class="token punctuation">(</span>CUDA<span class="token punctuation">)</span>include_directories<span class="token punctuation">(</span>/opt/ros/melodic/include<span class="token punctuation">)</span><span class="token comment"># include and link dirs of cuda and tensorrt, you need adapt them if yours are different</span><span class="token comment"># cuda</span>include_directories<span class="token punctuation">(</span>/usr/local/cuda/include<span class="token punctuation">)</span>link_directories<span class="token punctuation">(</span>/usr/local/cuda/lib64<span class="token punctuation">)</span><span class="token comment"># tensorrt</span><span class="token comment"># TODO(Call for PR): make TRT path configurable from command line</span>include_directories<span class="token punctuation">(</span>/home/nvidia/TensorRT-8.2.5.1/include/<span class="token punctuation">)</span>link_directories<span class="token punctuation">(</span>/home/nvidia/TensorRT-8.2.5.1/lib/<span class="token punctuation">)</span>include_directories<span class="token punctuation">(</span><span class="token variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/<span class="token punctuation">)</span>include_directories<span class="token punctuation">(</span><span class="token variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/plugin/<span class="token punctuation">)</span>file<span class="token punctuation">(</span>GLOB_RECURSE SRCS <span class="token variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/*.cpp <span class="token variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/src/*.cu<span class="token punctuation">)</span>file<span class="token punctuation">(</span>GLOB_RECURSE PLUGIN_SRCS <span class="token variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/plugin/*.cu<span class="token punctuation">)</span>add_library<span class="token punctuation">(</span>myplugins SHARED <span class="token variable">$&#123;PLUGIN_SRCS&#125;</span><span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>myplugins nvinfer cudart<span class="token punctuation">)</span>find_package<span class="token punctuation">(</span>OpenCV<span class="token punctuation">)</span>include_directories<span class="token punctuation">(</span><span class="token variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span><span class="token punctuation">)</span>add_executable<span class="token punctuation">(</span>yolov5_det yolov5_det.cpp <span class="token variable">$&#123;SRCS&#125;</span><span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>yolov5_det nvinfer<span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>yolov5_det cudart<span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>yolov5_det myplugins<span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>yolov5_det <span class="token variable">$&#123;OpenCV_LIBS&#125;</span><span class="token punctuation">)</span>target_link_libraries<span class="token punctuation">(</span>yolov5_det /opt/ros/melodic/lib/libserial.so<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相关解释：<br>add_executable(yolov5_det yolov5_det.cpp ${SRCS})</p><h3 id="核心代码：">核心代码：</h3><pre name="code" class="python">import torchfrom torchvision.utils import save_imageimport osfrom nets.tiny_unet_2_channelxiangdeng_RCABup3_1_3 import Decoderimport argparseimport torch.nn as nnfrom torchvision import transforms as Tfrom torchvision.datasets import ImageFolderfrom torch.utils import dataimport onnximport onnxruntimeclass Loader(nn.Module):    def __init__(self, image_dir, batch_size=1, num_workers=4):        super(Loader, self).__init__()        self.image_dir = image_dir        self.batch_size = batch_size        self.num_workers = num_workers    def forward(self):        transform = list()        transform.append(T.Resize([704, 1280]))  # 注意要为256的倍数        transform.append(T.ToTensor())        transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))        transform = T.Compose(transform)        dataset = ImageFolder(self.image_dir, transform)        data_loader = data.DataLoader(dataset=dataset,                                      batch_size=self.batch_size,                                      num_workers=self.num_workers)        return data_loaderclass Solver(object):    def __init__(self, data_loader, config):        self.data_loader = data_loader        self.global_g_dim = config.global_G_ngf        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        self.model_save_dir = config.model_save_dir        self.result_dir = config.result_dir        self.build_model()    def restore_model(self, resume_epoch):        """Restore the trained generator and discriminator."""        print('Loading the trained models from step {}...'.format(resume_epoch))        generator_path = os.path.join(self.model_save_dir, '{}-global_G.ckpt'.format(resume_epoch))        self.generator.load_state_dict(torch.load(generator_path, map_location=lambda storage, loc: storage))    def build_model(self):        """create a generator and a discrimin"""        self.generator = Decoder(self.global_g_dim)        self.generator.to(self.device)    def denorm(self, x):        out = (x + 1) / 2        return out.clamp_(0, 1)    def to_numpy(self, tensor):        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()    def test(self):        begin_epoch = 91        end_epoch = 92        step = 1        for k in range(begin_epoch, end_epoch, step):            self.restore_model(94)            self.generator.train()            modelData = "../model/demo" + str(k) + ".onnx"            # modelData = "demo_distok-1ok" + str(k) + ".onnx"            data_loader = self.data_loader            # with torch.no_grad():            for i, (val_img, _) in enumerate(data_loader):  # 出来的图片是RGB格式, 用PIL读取的                if i % 50 != 0:                    continue                x_real_name = data_loader.batch_sampler.sampler.data_source.imgs[i][0]                basename = os.path.basename(x_real_name)                result_path = os.path.join(self.result_dir, str(k))  # 生成的图片的保存文件夹                if not os.path.exists(result_path):                    os.makedirs(result_path)                result_path2 = os.path.join(result_path, basename)                distored_val = val_img.to(self.device)                clean_fake1 = self.generator(distored_val)                torch.cuda.synchronize()                clean_fake1 = clean_fake1.data.cpu()                save_image(self.denorm(clean_fake1), result_path2, nrow=1, padding=0)                torch.onnx.export(self.generator, distored_val, modelData, opset_version=9)                onnx_model = onnx.load(modelData)                onnx.checker.check_model(onnx_model)                ort_session = onnxruntime.InferenceSession(modelData,                                                            providers=['CUDAExecutionProvider'])                ort_inputs = {ort_session.get_inputs()[0].name: self.to_numpy(distored_val)}                ort_outs = ort_session.run(None, ort_inputs)                result_path1 = os.path.join(self.result_dir, "trt"+str(k))                result_pathtrt = os.path.join(result_path1, basename)                #save_image((self.denorm(torch.tensor(ort_outs[0]))),result_pathtrt, nrow=1, padding=0)                result = [val_img, torch.tensor(ort_outs[0]), clean_fake1]                result_concat1 = torch.cat(result, dim=3)                # break                save_image((self.denorm(result_concat1.data.cpu())), result_path2, nrow=2, padding=0)def main(config):    data_loader = Loader(config.data_dir)    solver = Solver(data_loader(), config)    solver.test()    #solver.build_model()    #solver.printnetwork()if __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument('--global_G_ngf', type=int, default=16, help='number of conv filters in the first layer of G')    parser.add_argument('--num_workers', type=int, default=4)    parser.add_argument('--data_dir', type=str,                        default=r'D:\Data\Paired\underwater_imagenet\val')    parser.add_argument('--model_save_dir', type=str,                        #default='/home/ouc/Li/Torch2TRT/trt_outfile/model'                        default=r'C:\TensorRT-8.6.0.12\samples\python\zzzzFSpiral\modules')    parser.add_argument('--result_dir', type=str,                        default=r'C:\TensorRT-8.6.0.12\samples\python\zzzzFSpiral\result')    torch.cuda.set_device(0)    config = parser.parse_args()    main(config)</pre><p>当pytorch转成onnx时，需要将模型进行eval模式，因为在PyTorch中，模型训练和推理（inference）有两种模式：训练模式和评估模式（evaluation mode）。在训练模式下，模型会保留一些中间结果，以便进行反向传播和参数更新(如batchnorm2d在train和eval模型中的计算不一致，涉及到内部的变量running_mean和running_eval的计算（如下图所示）)。而在评估模式下，模型不会保留这些中间结果，以便进行推理。因此，在将PyTorch模型转换为ONNX模型时，需要将模型切换到评估模式，以确保模型的输出与推理结果一致。如果在转换模型之前不将模型切换到评估模式，可能会导致转换后的ONNX模型输出与预期不一致。因此，为了确保转换后的ONNX模型的正确性，需要将PyTorch模型切换到评估模式（eval mode）再进行转换。</p><pre name="code" class="python">class MyBatchNorm2d(nn.BatchNorm2d):    def __init__(self, num_features, eps=1e-5, momentum=0.1,                 affine=True, track_running_stats=True):        super(MyBatchNorm2d, self).__init__(            num_features, eps, momentum, affine, track_running_stats)    def forward(self, input):        self._check_input_dim(input)        exponential_average_factor = 0.0        if self.training and self.track_running_stats:            if self.num_batches_tracked is not None:                self.num_batches_tracked += 1                if self.momentum is None:  # use cumulative moving average                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)                else:  # use exponential moving average                    exponential_average_factor = self.momentum        # calculate running estimates        if self.training:            mean = input.mean([0, 2, 3])            # use biased var in train            var = input.var([0, 2, 3], unbiased=False)            n = input.numel() / input.size(1)            with torch.no_grad():                self.running_mean = exponential_average_factor * mean\                    + (1 - exponential_average_factor) * self.running_mean                # update running_var with unbiased var                self.running_var = exponential_average_factor * var * n / (n - 1)\                    + (1 - exponential_average_factor) * self.running_var        else:            mean = self.running_mean            var = self.running_var        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))        if self.affine:            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]        return input</pre><p>当模型是train时，其<code>mean = input.mean([0, 2, 3])；var = input.var([0, 2, 3], unbiased=False)</code>，计算与当前的输入相挂钩，但是当模型是eval模式时，其<code>mean = self.running_mean；var = self.running_var</code>，计算与当前的输入无关，而是将学习到的running_mean与running_var直接作为当前计算的mean与var，由于该学习得到的均值与方差（running_mean与running_var）并不能完全适用所有的输入样例，因此模型在eval模式下的处理效果极大降低。</p><p>但是有人可能会想为什么模型转成onnx时要将模型设置为eval模式，直接设置为train模式不就可以解决处理效果下降的问题了吗？但是当你按该想法实验的时候，会发现虽然pytorch模型的处理效果没问题，但是onnx模型的处理效果还是一样的差，这又是为什么呢？我的猜测是onnx模型在推理时默认将batchnorm的模式设置为eval，进而输出的结果就存在问题了。这个是我个人的想法，大家有不同意见可以互相交流。</p><p>所以现在的问题就转换到了如何使模型在eval模式下计算的mean与var和当前输入相关，而不是依赖学习到的running_mean与running_eval。部分人可能遇到此问题就无从下手了，认为该问题是无法解决的。但是这个问题在我这里是能解决的。首先经过上述对问题的描述，大家对这个问题已经是比较清楚了。</p><p>接下来我将介绍两种很简单的方法来解决onnx模型处理效果差的问题：</p><h3 id="方法一：">方法一：</h3><h4 id="具体措施">具体措施:</h4><p>修改batchnorm2d中的源码，在if self.training：上方加一行self.training =True。这样尽管处于eval模式，但是batchnorm仍然是处于训练状态的，在这种情况下，mean与var自然就和input挂钩了。<br><img src="/images/Pytorch2trt/image-2.png" style="zoom: 200%;" /></p><h4 id="利弊：">利弊：</h4><p>不过这种方法虽然可以解决onnx模型处理效果差的问题，但是由该方法获得的onnx模型是无法成功转tensorrt的。原因是因为onnx模型的参数都是固定死的，正如上述我所展示的代码中，当模型处于训练模式时，running_mean与running_var是变化的，因此获得的onnx尽管输出没问题，但其内部是存在问题的。<br>batchnorm2d的源代码路径如下:<br><code>C:\Users\Spring\Anaconda3\envs\torchli\Lib\site-packages\torch\nn\modules\batchnorm.py</code></p><h3 id="方法二">方法二</h3><h4 id="具体措施-2">具体措施:</h4><p>修改batchnorm2d的源码，不过与方法一有所区别，具体修改见下图，之所以这样修改是因为batchnorm在eval模式下，mean与var和running_mean与running_val挂钩，故我们提前将学习到的running_mean与running_var设置成与input挂钩，然后就mean与var就和input联系起来了。<br><img src="/images/Pytorch2trt/image-20230514162311051.png" style="zoom: 200%;" /></p><h4 id="利弊：-2">利弊：</h4><p>不过这种方法虽然可以解决onnx模型处理效果差的问题，但是当可视化由该方法获得的onnx模型时，会发现batchnorm是不纯净的，上方多了一些计算分支（如右图所示），但是方法一的batchnorm是纯净的（如左图所示）。好处是由该方法获得的onnx模型是可以顺利转成trt模型的。<br><img src="/images/Pytorch2trt/image-1.png" style="zoom:150%;" /></p><h2 id="二、onnx-to-trt">二、onnx to trt</h2><h3 id="终端转换命令：">终端转换命令：</h3><p>Windows<br><code>.\trtexec.exe --onnx=aaa.onnx --saveEngine=retinate_hat_hair_beard_sim.trt --device=0</code><br>Ubuntu<br><code>trtexec --onnx=aaa.onnx --saveEngine=retinate_hat_hair_beard_sim.trt --device=0</code></p><h3 id="使用trt模型进行推理：">使用trt模型进行推理：</h3><pre name="code" class="python">import torchfrom torch.autograd import Variableimport tensorrt as trtimport pycuda.driver as cudafrom torchvision.utils import save_imageimport osimport pycuda.gpuarray as gpuarrayimport pycuda.autoinitfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoaderfrom torchvision import transforms as Tfrom torchvision.datasets import ImageFolderimport timeimport numpy as npfrom torch.utils import dataimport torch.nn as nnimport time# 操作缓存，进行运算， 这个函数是通用的def infer(context, input_img, output_size, batch_size=1):    # Convert input data to Float32,这个类型要转换，不严会有好多报错    input_img = input_img.astype(np.float32)    # Create output array to receive data    output = np.empty(output_size, dtype=np.float32)    # output = np.empty(batch_size * output.nbytes)    # Allocate device memory    d_input = cuda.mem_alloc(batch_size * input_img.nbytes)    d_output = cuda.mem_alloc(batch_size * output.nbytes)    bindings = [int(d_input), int(d_output)]    stream = cuda.Stream()    # Transfer input data to device    cuda.memcpy_htod_async(d_input, input_img, stream)    # Execute model    context.execute_async(batch_size, bindings, stream.handle, None)    # Transfer predictions back    cuda.memcpy_dtoh_async(output, d_output, stream)    stream.synchronize()    # Return predictions    return outputclass Loader(nn.Module):    def __init__(self, image_dir, batch_size=1, num_workers=4):        super(Loader, self).__init__()        self.image_dir = image_dir        self.batch_size = batch_size        self.num_workers = num_workers    def forward(self):        transform = list()        transform.append(T.Resize([704, 1280]))  # 注意要为256的倍数        transform.append(T.ToTensor())        transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))        transform = T.Compose(transform)        dataset = ImageFolder(self.image_dir, transform)        data_loader = data.DataLoader(dataset=dataset,                                      batch_size=self.batch_size,                                      num_workers=self.num_workers)        return data_loaderdef denorm(x):    out = (x + 1) / 2    return out.clamp_(0, 1)# 执行测试函数def do_test(context, batch_size=1):    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    test_loader = Loader(image_dir='/home/ouc/LiModel/data-paired/underwater_imagenet/val/')    print("mnist data load successful!!!")    accurary = 0    start_time = time.time()    times = []    for i, (val_img, _) in enumerate(test_loader()):    # 开始测试        # img, label = data        x_real_name = test_loader().batch_sampler.sampler.data_source.imgs[i][0]        basename = os.path.basename(x_real_name)        img = val_img.numpy()    # 这个数据要从torch.Tensor转换成numpy格式的        # print(img)        # label = Variable(label, volatile=True)        # with torch.no_grad():        #     label = Variable(label)        start_time1 = time.time()        output = infer(context, img, img.shape, 1)        end_time1 = time.time()        print(end_time1 - start_time1)        times.append((end_time1 - start_time1))        result_trt = torch.tensor(output)        # print(result_trt.shape)        # print(val_img.shape)        result = [val_img, result_trt]        result_concat1 = torch.cat(result, dim=3)        save_image((denorm(result_concat1.data.cpu())), '/home/ouc/Desktop/python/zzzzFSpiral/result/trt/' + basename, nrow=2, padding=0)        #print(output)        # conf, pred = torch.max(torch.Tensor(output), -1)        # num_count = (pred == label).sum()        # accurary += num_count.data    end_time = time.time()    print((start_time - end_time)/len(test_loader()))    print(np.mean(times))    # print("Test Acc is {:.6f}".format(accurary / len(test_dataset())))    # return accurary/len(test_dataset), time.time() - start_timedef trt_infer():    # 读取.trt文件    def loadEngine2TensorRT(filepath):        G_LOGGER = trt.Logger(trt.Logger.WARNING)        # 反序列化引擎        with open(filepath, "rb") as f, trt.Runtime(G_LOGGER) as runtime:            engine = runtime.deserialize_cuda_engine(f.read())            return engine    trt_model_name = "./resnet3.trt"    engine = loadEngine2TensorRT(trt_model_name)    # 创建上下文    context = engine.create_execution_context()    print("Start TensorRT Test...")    do_test(context)    # print('INT8 acc: {}, need time: {}'.format(acc, times))if __name__ == '__main__':    trt_infer()</pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于如何使用Hexo搭建自己的Github网站</title>
      <link href="/2023/05/18/build_web/"/>
      <url>/2023/05/18/build_web/</url>
      
        <content type="html"><![CDATA[<h2 id="一、下载Git与Nodejs">一、下载Git与Nodejs</h2><p>这里可以自行参考网上教程下载</p><h2 id="二、使用hexo创建个人博客">二、使用hexo创建个人博客</h2><h3 id="2-1-创建Blog根目录">2.1 创建Blog根目录</h3><ol><li>在本地创建一个文件夹，此文件夹将用来管理你的个人博客网站，如下图，我再F盘创建了一个git_blog文件夹<br><img src="/images/build_hexo/image1.png" alt="image1"></li><li>然后打开Git bash并输入<code>cd F:/git/blog</code>，如下图<br><img src="/images/build_hexo/image2.png" alt="image2"></li><li>然后再输入命令</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> hexo-cli <span class="token parameter variable">-g</span>hexo init<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>其中hexo init命令可能会失败，可以多试几次，最好挂梯子运行<br>全部输入完后，可以发现当前文件夹下多出一些文件夹和相关配置文件。<br><img src="/images/build_hexo/image4.png" alt="image4"></p><ul><li>关于这些文件夹，可以先做一个简单的介绍：<ul><li>node_modules: 依赖包</li><li>public：存放生成的页面</li><li>scaffolds：生成文章的一些模板</li><li>source：用来存放你的文章</li><li>themes：主题</li></ul></li></ul><ol start="4"><li>然后输入命令：</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span>hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>完成后会显示如下内容<br><img src="/images/build_hexo/image5.png" alt="image5"><br>在浏览器中输入<a href="http://localhost:4000/">http://localhost:4000/</a>后可以进入Hexo传统页面，如下<br><img src="/images/build_hexo/image3.png" alt="image3"></p><h3 id="2-2-Github创建仓库">2.2 Github创建仓库</h3><ol><li>首先需要一个Github账号，关于创建Github账号，这里就略去了</li><li>创建一个Github仓库，需要注意的是，仓库的名称必须为你的Github用户名，如下：因为我的用户名为Sprli，<a href="http://xn--Sprli-dq1hu9a34zzzxxjp8t7c.github.io">故仓库名称为Sprli.github.io</a><br><img src="/images/build_hexo/image6.png" alt="image6"></li><li>创建完后回到Git Bash窗口，输入以下命令：</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> config <span class="token parameter variable">--global</span> user.name <span class="token string">"your name"</span> <span class="token comment"># 注意这里是你的Github用户名</span><span class="token function">git</span> config <span class="token parameter variable">--global</span> user.email <span class="token string">"your email"</span> <span class="token comment"># 注意这里是你的Github邮箱</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如下： 这里Sprli为我的Github用户名，9****.qq.com为我的邮箱<br><img src="/images/build_hexo/image7.png" alt="image7"></p><h3 id="2-3-创建ssh，便于链接Github">2.3 创建ssh，便于链接Github</h3><ol><li>创建ssh， 输入命令后一直回车即可<br><code>ssh-keygen -t rsa -C &quot;your email</code><br>之后会提示你已经完成ssh的创建，在文件夹中找到.ssh这个隐藏文件夹，其中Spring为我电脑本地户名<br><img src="/images/build_hexo/image8.png" alt="image8"><br>进入该文件夹后，可以发现文件夹下有如下文件：<br><img src="/images/build_hexo/image9.png" alt="image9"><br>记住上图圈红文件，在Github的setting中找到SSH keys，将id_rsa.pub中的内容全部复制到key中去，title内容随便填写就行<br><img src="/images/build_hexo/image10.png" alt="image10"><br><img src="/images/build_hexo/image12.png" alt="image12"><br><img src="/images/build_hexo/image13.png" alt="image13"><br>操作完成后，恭喜你，快成功了！！！</li></ol><h3 id="2-4-将hexo部署到Github">2.4 将hexo部署到Github</h3><p>在git_blog文件夹下找到_config.yml文件，这是属于你的博客的配置文件，可以在这里面直接修改 姓名，内容等用户的信息。（可以使用vscode或者记事本等编辑器都可以）</p><h4 id="2-4-1-修改其中的必要信息">2.4.1 修改其中的必要信息</h4><ol><li>修改其中的url为<code>https://username.github.io</code>，其中username为你Github用户名<br><img src="/images/build_hexo/image14.png" alt="image14"></li><li>将deploy部分修改为：</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Deployment</span><span class="token comment">## Docs: https://hexo.io/docs/one-command-deployment</span><span class="token comment"># deploy:</span><span class="token comment">#   type: ''</span>deploy:  type: <span class="token function">git</span>  repo: https://github.com/username/username.github.io.git  <span class="token comment"># 其中的username替换为你的Github用户名</span>  branch: ph-pages  <span class="token comment"># 注意branch不能为master，不然可能会出错</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/build_hexo/image15.png" alt="image15"><br>3. 安装git部署命令工具<br><code>npm install hexo-deployer-git --save</code><br>随后输入以下命令无错误后即完成在Github上的部署</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo cleanhexo ghexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>其中<code>hexo clean</code>清除了你之前生成的东西，也可以不加。<code>hexo generate</code>生成静态文章，可以用<code>hexo g</code>缩写，<code>hexo deploy</code>部署文章，可以用<code>hexo d</code>缩写。如果是在离线端即 localhost:4000端测试你的博客，则只需要<code>hexo g</code> + <code>hexo s</code>即可，无需<code>hexo d</code>，其中<code>hexo s</code>是<code>hexo server</code>的缩写。</p><h3 id="2-5-发布自己的博客">2.5 发布自己的博客</h3><p>在git_blog文件夹下找到source/_posts文件夹，在其中创建.md格式文件，然后在其中编写完自己的博客后，使用<code>hexo clean</code> + <code>hexo g</code> + <code>hexo d</code>即完成自己博客的编写与上传。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>关于Jetson Xavier NX 板子的系统烧录</title>
      <link href="/2023/05/18/Jetson%20NX/"/>
      <url>/2023/05/18/Jetson%20NX/</url>
      
        <content type="html"><![CDATA[<h2 id="一、准备">一、准备</h2><ol><li>Ubuntu电脑 or 虚拟机（Ubuntu系统）</li><li>能够进行数据传输的Micro-USB数据线</li><li>显示屏 、HDMI转接线、键鼠</li></ol><h2 id="二、进行烧录（参考瑞泰教程）">二、进行烧录（参考瑞泰教程）</h2><h3 id="2-1-系统软件包的下载">2.1 系统软件包的下载</h3><p>2.1.1 烧录所需文件集中在这两个文件夹中，在本部分我选择安装LT4 R32.7.1版本</p><img src="/images/Jetson/image-20230311205757974.png" alt="image-20230311205757974" style="zoom:50%;" /><p>2.1.2根据Jetson类型进行选择</p><img src="/images/Jetson/image-20230311205921697.png" alt="image-20230311205921697" style="zoom:50%;" /><p>2.1.3根据载板型号进行选择</p><img src="/images/Jetson/image-20230311210038630.png" alt="image-20230311210038630" style="zoom:50%;" /><p>2.1.4随便选择一个版本</p><img src="/images/Jetson/image-20230311210058553.png" alt="image-20230311210058553" style="zoom:50%;" /><p>2.1.5下载对应文件，其中rtso-6002对应位置即为载板型号</p><img src="/images/Jetson/image-20230311210118374.png" alt="image-20230311210118374" style="zoom:50%;" /><p>2.1.6选择对应版本的L4T文件</p><img src="/images/Jetson/image-20230311210156444.png" alt="image-20230311210156444" style="zoom:50%;" /><p>2.1.7下载相应文件</p><img src="/images/Jetson/image-20230311210216935.png" alt="image-20230311210216935" style="zoom:50%;" /><h3 id="2-2-在PC端Ubuntu系统进行烧录环境准备">2.2 在PC端Ubuntu系统进行烧录环境准备</h3><p>2.2.1 将上述文件拷贝至烧录主机同一目录下</p><p>2.2.2 解压 Linux Driver Package</p><p><code>$ tar -vxf Jetson_Linux_R32.7.1_aarch64.tbz2</code></p><p>2.2.3 设置根文件系统</p><ol><li><p>进入Linux Driver Package 的根文件系统目录</p><p><code>$ cd &lt;your_L4T_root&gt;/Linux_for_Tegra/roofs</code></p></li><li><p>解压 the Root File System ：</p><p><code>$ sudo tar -jxpf ../../Tegra_Linux_Sample-Root-Filesystem_R32.7.1_aarch64.tbz2</code></p></li></ol><p>2.2.4 安装 BSP 支持包</p><ol><li><p>将 Realtimes-L4T-<version>.tar 包解压到与 Linux_for_Tegra 文件夹同级目录下面，使用命令：</p><p><code>$ tar -xvf Realtimes-L4T-&lt;version&gt;.tar</code></p></li><li><p>进入到 Realtimes-L4T 文件夹，运行：</p><p><code>$ sudo ./install.sh</code><br>安装成功会有 success 提示！</p></li><li><p>运行 apply_binaries.sh 脚本拷贝 NVIDIA 用户空间库进入目标文件系统<br><code>$ cd ../Linux_for_Tegra/</code><br><code>$ sudo ./apply_binaries.sh</code></p></li></ol><h3 id="3-1-系统烧录">3.1 系统烧录</h3><p>3.3.1先c将板子进入recovery模式，然后将板子与主机通过Micro-USB数据线连接，在PC端的Linux_for_Tegra目录下进行如下操作：<br><code>$ sudo ./flash.sh realtimes/rtso-&lt;model&gt; mmcblk0p1</code><br>注意：rtso-<model>指的是板子型号，例如我使用的板子是rtso-6002E-v1.2，则命令为：<br><code>$ sudo ./flash.sh rtso-6002e-1.2v mmcblk0p1</code></p><p>3.3.2 可能遇到的问题</p><ol><li><p>板子型号查看，一般来说板子上会带有型号，若找不到可以咨询卖方</p><img src="/images/Jetson/image-20230311213021400.png" alt="image-20230311213021400" style="zoom:50%;" /></li><li><p>关于输入选项，如下，可以发现红框圈中的有很多型号的选项，其中rtso-6002e表示rtso-6002-emmc板，rtso-6002则是另一种类型的板子，并且6002-emmc下还有v1.2、v1.3等型号，根据板子进行选择。（一定不要弄错，这些文件是与主板相配的）</p></li></ol><p><img src="/images/Jetson/image-20230311214628664.png" alt="image-20230311214628664"></p><p>3.3.3 报错一</p><p><img src="/images/Jetson/image-20230311214653624.png" alt="image-20230311214653624"></p><p>如上报错需考虑两个原因：</p><ol><li>主机与板子进行数据传输的Micro-USB数据线是否具有传输数据的作用，可以使用lsusb查看是否含有Nvidia Crop；</li><li>板子是否进入了recovery模式。</li></ol><p>3.3.4 报错二</p><p><img src="/images/Jetson/image-20230319005550878.png" alt="image-20230319005550878"></p><p>仔细观察可以发现问题在于一个<code>.py</code>文件执行错误，可以使用命令<code>$ sudo apt install python2.7 python3 python</code>安装python，然后重新烧录解决。</p><h3 id="3-4-配置板子系统">3.4 配置板子系统</h3><p>由于瑞泰板子是上电自启动的，将板子和显示屏以及键鼠连接，进入类似Ubuntu系统开机配置界面，除了地区选择上海外，其余保持默认即可。能够正常开机，则表明烧录成功。随后可以发现，其图形化界面和Ubuntu几乎没有什么区别。</p><h2 id="三、进行系统迁移（视情况选择）">三、进行系统迁移（视情况选择）</h2><h3 id="3-1-方法一">3.1 方法一</h3><p>3.1.1 查看SSD设备名称<br>系统启动前，将SSD插入到板子的内存接口处（由于瑞泰板子自身装有一个120GB的内存卡，因此可以直接使用其自身的SSD卡即可）。系统启动后，使用<code>$ sudo fdisk -l</code>命令查看SSD设备名称，例如：nvme0n1、mmcblk1，本文使用mmcblk1为例。<br>3.1.2 对SSD进行格式化<br>如果SSD之前没有进行格式化，需要把SSD格式化后再使用。对于已挂载的SSD卡，需要使用umount卸载SSD卡，再格式化。卸载命令：<code>sudo umount /dev/mmcblk1</code>；格式化命令：<code>$ sudo mkfs.ext4 /dev/mmcblk1</code><br>3.1.3 创建一个新的GPT分区<br><code>$ sudo parted /dev/mmcblk1 mklabel gpt</code><br>3.1.4 添加分区<br><code>$ sudo parted /dev/mmcblk1 mkpart primary 0GB &lt;Size&gt;</code><br>Size是分区的大小，最小8GB，建议可以将SSD卡内存全部添入<br>例如：准备分区的大小是50GB，则命令是：<br><code>$ sudo parted /dev/mmcblk1 mkpart primary 0GB 50GB</code><br>添加完分区后，使用<code>$ sudo fdisk -l</code>可以看到mmcblk1下新增一个分区，名为：mmcblk1p1</p><p>3.1.5 格式化分区<br><code>$ sudo mkfs.ext4 /dev/mmcblk1p1</code><br>把分区格式化为ext4 格式<br>3.1.6 拷贝roofs到SSD<br><code>$ sudo dd if=/dev/mmcblk0p1 of=/dev/mmcblk1p1 bs=1M</code><br>其中mmcblk0p1是系统原先所在位置，mmcblk1p1则是我们的转移目标区域<br>3.1.7 修复分区<br><code>$ sudo -s</code><br><code>$ fsck.ext4 /dev/mmcblk1p1</code><br>若遇到输入yes or no，请全部输入yes</p><p>3.1.8 调整系统分区大小<br><code>$ sudo resize2fs /dev/mmcblk1p1</code></p><p>3.1.9 烧写从SD卡启动系统<br><code>$ sudo ./flash.sh realtimes/rtso-&lt;model&gt; mmcblk1p1</code><br>这一步骤和烧录步骤一摸一样，需要注意的是目录位置、烧录系统文件与最开始烧录时是一样的，且目标位置名称变化了<br>重新烧录完后，进入系统后，输入<code>$ df -h</code>可以看到mmcblk1p1成为根目录，系统已从SD卡启动。</p><h3 id="3-2-方法二">3.2 方法二</h3><p>[^参考博客]: <a href="http://t.csdn.cn/Ne0Bk">http://t.csdn.cn/Ne0Bk</a>  <a href="http://t.csdn.cn/ZrDlI">http://t.csdn.cn/ZrDlI</a></p><p>3.2.1 将SD卡进行格式化操作并挂载到系统下，操作如下：<br>系统迁移的前提是将SD卡挂载到系统下，先需要使用如下命令将SD卡格式化为EXT4格式：<br><code>$ sudo mke2fs -t ext4 /dev/mmcblk1p1</code><br>然后将SD卡挂载到/mnt下：<br><code>$ sudo mount /dev/mmcblk1p1 /mnt</code></p><p>3.2.2 克隆所需文件：</p><p><code>$ git clone https://github.com/jetsonhacks/rootOnNVMe.git</code></p><p>3.2.3 进入下载的文件夹中，<a href="http://xn--copy-rootfs-ssd-gm6x791zrggm36j4oi1j3f.sh">编辑脚本文件copy-rootfs-ssd.sh</a>，然后运行该脚本，将系统复制到SSD中。<br><code>cd ./rootOnNVMe</code><br><code>sudo gedit ./copy-rootfs-ssd.sh</code><br>将文件中的<code>/dev/nvme0n1p1 </code>修改为<code>/dev/mmcblk1</code><br><code>sudo ./copy-rootfs-ssd.sh</code></p><img src="/images/Jetson/image-20230311225106076.png" alt="image-20230311225106076" style="zoom: 80%;" /><p>3.2.4 进入到<code>rootOnNVMe/data</code>目录下，修改其中的脚本文件:<code>setssdroot.sh</code>与<code>setssdroot.service</code>，将其中的<code>/dev/nvme0n1p1</code>修改为<code>/dev/mmcblk1</code>，方法同上。</p><p><img src="/images/Jetson/image-20230311225229373.png" alt="image-20230311225229373"></p><p>3.2.5 回到rootOnNVMe目录下，执行以下命令：<br><code>sudo ./setup-service.sh</code></p><p>成功后如下图：</p><img src="/images/Jetson/image-20230311225311334.png" alt="image-20230311225311334" style="zoom:50%;" /><p>3.2.6 重新启动jetson板后，使用命令<code>$ df -h</code>查看，可以发现系统已经完成迁移</p><h2 id="四、安装Jepack">四、安装Jepack</h2><h3 id="4-1、基本步骤">4.1、基本步骤</h3><p>4.1.1 安装前信息确认以及更新软件源（在板子上进行）<br>给Xavier NX安装软件之前需先确定jetson设备系统l4t版本，因为NVDIA jetpack跟该版本号具有一定的对应关系，如果版本号不对应会导致出现一些异常。具体的对应关系可以参考Jetpack的说明：</p><p>在Xavier NX设备上使用以下命令可以查看系统的L4T版本号：<br><code>$ head–n 1 /etc/nv_tegra_release</code></p><p>4.1.2 进入载板系统，打开SysteamSettings–&gt;Software&amp;Updates&gt;Ubuntu Software</p><p>4.1.3 下载安装Jetpack/sdkmanager并运行（在PC端Ubuntu系统上进行）<br>[sdkmanager下载网址]: <a href="https://developer.nvidia.com/drive/sdk-manager">https://developer.nvidia.com/drive/sdk-manager</a></p><p>选择安装最新版本的sdkmanager下载安装<br>安装命令：<code>$ sudo dpkg -i sdkmage&lt;……&gt;.deb</code> or <code>$ sudo apt install ./sdkmanger-&lt;……&gt;.deb</code><br>运行命令：<code>$ sdkmanager</code></p><p>运行界面和操作如下：</p><ol><li><p>登录界面（低版本可能无法登录）</p><img src="/images/Jetson/image-20230311232026679.png" alt="image-20230311232026679" style="zoom:50%;" /></li><li><p>点击圈中的1、2，进行选择Jetson类型以及与LT4相对应的Jetpack版本，3取消勾选。如果2中未发现与其对应的Jetpack版本，则运行sdkmanager的命令改为：<code>$ sdkmanager --archivedversions</code>，其它操作不变。</p><img src="/images/Jetson/image-20230311232300525.png" alt="image-20230311232300525" style="zoom:50%;" /></li><li><p>由于以及安装过系统，圈中的1取消勾选，圈中的2可以选择合适的位置（默认不变），圈中的3注意取消勾选<strong>Download now.Install later.<strong>点击</strong>continue</strong>。</p><img src="/images/Jetson/image-20230311233159191.png" alt="image-20230311232542687" style="zoom:50%;" /></li><li><p>输入PC端Ubuntu主机密码</p><img src="/images/Jetson/image-20230311232559144.png" alt="image-20230311232559144" style="zoom:50%;" /></li><li><p>圈中的1，实际上应是显示有设备连接的；圈中的2填写<strong>板子Ubuntu系统</strong>的Username和Password；随后点击Install。</p><img src="/images/Jetson/image-20230311233500272.png" alt="image-20230311233500272" style="zoom:50%;" /></li><li><p>接着进入以下界面，这些指标都是板子的指标，如内存是否足够、网络是否连接等。</p><p><img src="/images/Jetson/image-20230311234633908.png" alt="image-20230311234633908"></p><p>7.最后出现接下来画面即成功</p><p><img src="/images/Jetson/image-20230319113455316.png" alt="image-20230319113455316"></p></li></ol><h3 id="4-2-问题及解决">4.2 问题及解决</h3><p>4.2.1 如上图，如果出现Internet connection问题，按照其指示可以采取如下方法解决：</p><ol><li>在jetson板上终端输入：<code>$ ping -c 3 www.nvidia.com</code></li></ol><p><img src="/images/Jetson/image-20230311234239264.png" alt="image-20230311234239264"></p><ol start="2"><li><p>将出现的Ip地址放到jetson板子的/etc/hosts文件中</p></li><li><p>使用命令：<code>$ sudo gedit /etc/hosts</code></p></li></ol><p>​       在文件中添加一行：<code>23.48.214.59 nvidia.com</code></p><ol start="4"><li>保存后点击Retry即可。</li></ol><p>如果上述方法行不通，则可能是sdkmanager<strong>版本不是最新</strong>造成的。</p><p>4.2.2 若出现第三个Accept to APT……错误，则可能是jetson板上的源未更新造成的。</p><ol><li>可以在jetson板上依次输入命令：</li></ol><p><code>$ sudo apt-get update</code> &amp;<code>$ sudo apt-get upgrade</code></p><ol start="2"><li><p>更新完后Retry即可。</p></li><li><p>有时光多Retry几次可能就会成功。</p></li></ol><p>若上述操作失效，则可以替换Jetson板上的Ubuntu源（注意是<strong>Jetson源</strong>），重新更新尝试一下；或者退出sdkmanager重新弄一遍。</p><h2 id="五、安装系统所需的包">五、安装系统所需的包</h2><h3 id="5-1-配置cuda环境变量">5.1 配置cuda环境变量</h3><p><code>vi ~/.bashrc  # 打开/.bashrc文件</code><br>将以下内容输入该文件中（末尾即可）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>/usr/local/cuda10.2/bin:<span class="token environment constant">$PATH</span> <span class="token comment"># cuda后跟的是cuda版本</span><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span>/usr/local/cuda10.2/lib64:<span class="token variable">$LD_LIBRARY_PATH</span><span class="token builtin class-name">export</span> <span class="token assign-left variable">CUDA_ROOT</span><span class="token operator">=</span>/usr/local/cuda10.2  <span class="token comment"># 这一项不是很确定是否有用</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="5-2-配置系统级安装包">5.2 配置系统级安装包</h3><p>输入以下命令即可</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> build-essential <span class="token function">make</span> cmake cmake-curses-gui<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span> g++ pkg-config <span class="token function">curl</span>  <span class="token function">zip</span> zlib1g-dev libopenblas-base libopenmpi-dev <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libatlas-base-dev gfortran libcanberra-gtk-module libcanberra-gtk3-module<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libhdf5-serial-dev hdf5-tools libhdf5-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">nano</span> <span class="token function">locate</span> <span class="token function">screen</span><span class="token comment">#scipy 依赖和 cython</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libfreetype6-dev <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> protobuf-compiler libprotobuf-dev openssl<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libssl-dev libcurl4-openssl-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cython3<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libxml2-dev libxslt1-dev<span class="token comment">#opencv 依赖</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libavcodec-dev libavformat-dev libswscale-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libxvidcore-dev libavresample-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libtiff-dev libjpeg-dev libpng-dev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="5-3-安装配置Python">5.3 安装配置Python</h3><p>5.3.1 默认安装python3.6版本</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> python3-dev python3-testresources python3-setuptools<span class="token function">wget</span> https://bootstrap.pypa.io/get-pip.py<span class="token function">sudo</span> python3 get-pip.py <span class="token comment"># 该命令可能会失败，显示python3使用的python3.6版本较低，导致不成功</span><span class="token function">rm</span> get-pip.py <span class="token comment"># 然后换pip源</span><span class="token function">mkdir</span> ~/.pip<span class="token function">vim</span> ~/.pip/pip.conf <span class="token comment"># 换清华源</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>粘贴以下内容到该文件<br><code>[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></p><p>5.3.2 解决sudo python3 get-pip.py失败，这里我选择使用python3.7版本</p><ol><li>安装python3.7</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update  <span class="token comment"># 更新(可跳过)</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python3.7python3.7 <span class="token parameter variable">-V</span>  <span class="token comment">#查看是否安装成功</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>创建软连接，使python3默认指向python3.7</li></ol><ul><li>首先把之前的软连接删除：</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">rm</span> <span class="token parameter variable">-rf</span> /usr/bin/python3 <span class="token function">which</span> python3.7  <span class="token comment"># 查看python3.7 安装路径.这里输出/usr/bin/python3.7</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>创建新的软连接：</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /usr/bin/python3.7 /usr/bin/python3  <span class="token comment"># 添加python3的软链接。 /usr/bin/python3.7 即 which python3.7输出的安装路径</span>python3 <span class="token parameter variable">-V</span>  <span class="token comment"># 测试是否安装成功</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="3"><li>切换python版本，<br><a href="jetson%E5%8F%AF%E4%BB%A5%E5%AE%89%E8%A3%85minconda%E6%9D%A5%E4%BD%BF%E7%94%A8conda%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%EF%BC%8C%E4%BD%86%E6%98%AF%E4%B8%8D%E5%A6%82%E5%AE%89%E8%A3%85virtualenv%E6%9D%A5%E7%9A%84%E6%96%B9%E4%BE%BF%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8D%A0%E5%86%85%E5%AD%98%E4%B9%9F%E4%B8%8D%E5%B0%8F">^解释</a>: 对于不同版本的jetpack，其cuda对应的python版本有所不同，使用该方法便于配合cuda的使用</li></ol><ul><li>首先查看python文件<br><code>ls /usr/bin/python* # 查看python文件，会出现python2.7、python3.6、python3.7三个版本</code></li><li>首先看看系统是否配置过python相关的管理信息<br><code>update-alternatives --list python # 如果显示：no alternatives for python，表示没有配置过。</code></li><li>使用以下命令进行配置：</li></ul><pre name="code" class="python">sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.7 3</pre><ul><li>配置成功，会提示你<code> in auto mode</code>.这个时候我们再次查看配置<br><code>update-alternatives --list python</code></li><li>会看到我们的三个版本已经成功进行了配置。如果需要切换版本，输入命令<br><code>sudo update-alternatives --config python #  然后输入相应的版本序号即可</code></li><li>最后使用：<code>python --version</code>查看版本号，可以发现配置成功</li></ul><h3 id="5-4-安装jtop管理GPU">5.4 安装jtop管理GPU</h3><p>输入命令安装jtop：<br><code>sudo -H pip install jetson-stats</code><br>安装成功后，输入<code>sudo jtop</code>即可查看硬件相关信息</p><h3 id="5-5-安装virtualenv来创建虚拟环境">5.5 安装virtualenv来创建虚拟环境</h3><ol><li>安装与配置</li></ol><ul><li>使用pip下载virtualenv</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> pip <span class="token function">install</span> virtualenv virtualenvwrapper<span class="token function">vim</span> ~/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>将以下内容输入其中：</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#virtualenv and virtualenvwrapper</span><span class="token builtin class-name">export</span> <span class="token assign-left variable">WORKON_HOME</span><span class="token operator">=</span><span class="token environment constant">$HOME</span>/.virtualenvs <span class="token comment">#指定所有的需拟环境的安装位置</span><span class="token builtin class-name">export</span> <span class="token assign-left variable">VIRTUALENVWRAPPER_PYTHON</span><span class="token operator">=</span>/usr/bin/python3.6 <span class="token comment">#指定解释器，改为cuda对应的python版本</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>终端输入指令激活virtualenv</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token environment constant">$HOME</span>/.virtualenvs<span class="token builtin class-name">source</span> /usr/local/bin/virtualenvwrapper.sh <span class="token comment">#进行激活生效</span><span class="token builtin class-name">source</span> ~/.bashrc <span class="token comment">#重新载入</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ol start="2"><li>virtualenv的使用相关操作</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mkvirtualenv name <span class="token comment"># 创建一个环境</span>mkvirtualenv <span class="token parameter variable">-p</span> /python目录/python.exe name <span class="token comment"># 不使用默认python版本、使用指定python版本创建环境</span>workon name <span class="token comment"># 激活环境</span>deactivate <span class="token comment"># 退出</span>rmvirtualenv name <span class="token comment"># 删除</span>lsvirtualenv <span class="token comment"># 所有环境</span>cpvirtualenv source_name dest_name <span class="token comment"># 复制虚拟环境</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="六、安装pytorch以及torchvision">六、安装pytorch以及torchvision</h2><h3 id="6-1-基本操作">6.1 基本操作</h3><ol><li>创建虚拟环境<br>[^声明]: 以创建虚拟环境名位torchli，安装1.10.0版本的pytorch和对应版本0.11.1的torchvision为例</li></ol><ul><li>使用以下命令创建虚拟环境</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Mkvirtualenv <span class="token parameter variable">-p</span> /usr/bin/python3.6 torchli <span class="token comment"># -p 后面跟着的是制定python的版本，因为有的pytorch对python版本有要求，所以要指定版本</span>workon torchli <span class="token comment"># 进入虚拟环境</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>如果遇见<code>workon command not find</code>，则使用以下命令<br><code>Source virtualenvwrapper.sh</code></li><li>然后再<code>workon torchli</code></li></ul><ol start="2"><li>安装pytorch</li></ol><ul><li>首先下载pytorch pip wheels（对应自己装的Jatpack的版本）<br>[下载网址]:<a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048">https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048</a></li><li>下载完后，输入命令：</li></ul><pre name="code" class="python">pip install torch-1.10.0-cp36-cp36m-linux_aarch64.whl # 安装pytorchgit clone --branch v0.11.1 https://github.com/pytorch/vision torchvision # 下载0.11.1版本cd torchvisionexport BUILD_VERSION=0.11.1python setup.py install --user # --user可加可不加</pre><ol start="3"><li>验证是否安装成功</li></ol><ul><li>上述操作完成后，输入命令：</li></ul><pre name="code" class="python">python # 进入python：import torch # 导入torch库torch.cuda.is_available()  # 验证cuda是否能使用，输出应为True，则pytorch安装成功import torchvision # 导入torchvision库print(Torchvision.__version__) # 打印版本号，未报错即成功</pre><h3 id="6-2-问题及解决">6.2 问题及解决</h3><p>6.2.1. 激活环境失败，报错输出：<code>workon command not find</code><br>输入命令<code>source virtualenvwrapper.sh # 输入之后再尝试激活即可</code><br>6.2.2. import torch时报错,Illegal instruction (core dumped)</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 修改环境变量</span><span class="token function">sudo</span> gedit /etc/profile <span class="token builtin class-name">export</span> <span class="token assign-left variable">OPENBLAS_CORETYPE</span><span class="token operator">=</span>ARMV8 <span class="token comment"># 将其加入最后面一行，然后保存</span><span class="token comment"># 更新环境变量</span><span class="token builtin class-name">source</span> /etc/profile<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>6.2.3. import torchvision报错<br>[该报错大致可以分为两个问题]: 1、pillow和torchvision版本不对应出错；2、当前路径含有同名文件导致报错</p><ol><li>在import torchvision过程中遇到有关PIL库的提示报错<br>解决方法：尝试卸载当前的pillow，安装较低版本的pillow<br>输入命令：<pre name="code" class="python">pip uninstall pillowpip install pillow==6.1 # 一般来说安装此版本即可解决问题</pre></li><li>由于当前在torchvision文件夹下安装的torchvision，若不退出此路径也会导致import torchvision报错，报错提示路径问题<br>解决方法：退出当前文件夹路径，输入以下命令：<br><code>cd .. # 返回上级路径，然后再次尝试import torchvision</code></li></ol><h2 id="七、安装opencv">七、安装opencv</h2><h3 id="7-1-情形一">7.1 情形一</h3><p><a href="%E5%A6%82%E6%9E%9C%E5%9C%A8%E5%AE%89%E8%A3%85jetpack%E6%97%B6%E6%9C%AA%E5%AE%89%E8%A3%85opencv%EF%BC%8C%E6%88%96%E8%80%85%E6%98%AF%E6%83%B3%E6%8D%A2%E6%96%B0%E7%9A%84%E7%89%88%E6%9C%AC%EF%BC%8C%E5%A6%823.4.5%E3%80%82%E5%88%99%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E4%BB%A5%E4%B8%8B%E6%96%B9%E6%B3%95%E5%AE%89%E8%A3%85opencv">^具体阐述</a>: 在新创建的python虚拟环境中，无法import cv2，且pip install opencv-python始终无法完成，故需要安装opencv，但是一般来说，安装jetpack后，自动安装的有opencv，若此opencv版本不影响你的代码，那么就可以进行以下操作便捷使用opencv库了</p><ol><li>首先在终端执行以下指令查找编译好的cv2库文件的路径</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">find</span> / <span class="token parameter variable">-iname</span> <span class="token string">"*cv2*"</span> <span class="token comment"># 得到类似路径/usr/lib/python3.6/dist-packages/cv2/python-3.6/cv2.cython-36m-aarch64-linux-gnu.so，重点是cv2.cython-36m-aarch64-linux-gnu.so文件要求一致</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>随后进入虚拟环境的site-packages文件夹下,并链接到查找到的cv2库文件路径即可</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> /home/nx407/.virtualenv/torchli/lib/python3.6/site-packages <span class="token comment"># 其中nx407是用户名、torchli是我建立的虚拟环境名</span><span class="token function">ln</span> <span class="token parameter variable">-s</span> /usr/lib/python3.6/dist-packages/cv2/python-3.6/cv2.cython-36m-aarch64-linux-gnu.so cv2.so  <span class="token comment"># 注意ln，link的意思，不是大写i</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="3"><li>安装完成后，在虚拟环境中执行下列指令以确保python能正确调用cv2。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python">python               <span class="token operator">//</span>进入python<span class="token keyword">import</span> cv2cv2<span class="token punctuation">.</span>__version__    <span class="token operator">//</span>若安装成功且能正常调用，此处能输出安装的从v版本quit<span class="token punctuation">(</span><span class="token punctuation">)</span>               <span class="token operator">//</span>退出python<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="7-2-情形二">7.2 情形二</h3><ol><li><p>查看已预安装Opencv版本<br><code>pkg-config --modversion opencv</code></p></li><li><p>卸载原Opencv</p></li></ol><ul><li>如果是自己之前安装的话，就找到当初安装Opencv的build文件夹路径，然后进入该build目录执行卸载操作：</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> <span class="token punctuation">..</span>./opencv-4.x.x/build<span class="token function">sudo</span> <span class="token function">make</span> uninstall<span class="token builtin class-name">cd</span> <span class="token punctuation">..</span>./opencv-4.x.x<span class="token function">sudo</span> <span class="token function">rm</span> <span class="token parameter variable">-r</span> build<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>若不是则执行以下操作：</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> purge libopencv*<span class="token function">sudo</span> <span class="token function">apt-get</span> purge python-numpy<span class="token function">sudo</span> <span class="token function">apt</span> autoremove // 删除其他未使用的apt包，可有可无<span class="token function">sudo</span> <span class="token function">apt-get</span> update<span class="token function">sudo</span> <span class="token function">apt-get</span> dist-upgrade<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="3"><li><p>下载Opencv3.4.5.zip文件</p></li><li><p>安装一些相关库和包</p></li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> installbuild-essential <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cmake <span class="token function">git</span> libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> --only-upgrade g++-5 cpp-5 gcc-5<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>如果上述指令失败的话，可以尝试网上换源，如下<br>更新后再安装Qt5<br><code>sudo apt-get install qt5-default # qt5必须要安装成功，不过丢包概率不大，都能装上的</code></p><ol start="5"><li><p>CUDA部分源码的修改<br>先找到<code>cuda_gl_interop.h</code>文件，一般都是在<code>/usr/local/cuda/include</code>里<br>然后在命令框输入<br><code>sudo gedit /usr/local/cuda/include/cuda_gl_interop.h</code><br>这时会弹出一个文件如下图<br><img src="/images/Jetson/image-20230322013045837.png" alt="image-20230322013045837"><br>找到红框内代码，并按照图中代码更改为以上内容，然后点save保存，退出。</p></li><li><p>开始编译安装opencv-3.4.5<br>找到opencv-3.4.5所在文件夹（即前文解压后的文件夹），然后cd到build文件夹里面。<br>控制台输入：</p></li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cmake <span class="token parameter variable">-D</span> <span class="token assign-left variable">CMAKE_BUILD_TYPE</span><span class="token operator">=</span>RELEASE <span class="token parameter variable">-D</span> <span class="token assign-left variable">CMAKE_INSTALL_PREFIX</span><span class="token operator">=</span>/usr/local <span class="token punctuation">\</span>      <span class="token parameter variable">-D</span> <span class="token assign-left variable">WITH_CUDA</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">CUDA_ARCH_BIN</span><span class="token operator">=</span><span class="token string">"6.2"</span> <span class="token parameter variable">-D</span> <span class="token assign-left variable">CUDA_ARCH_PTX</span><span class="token operator">=</span><span class="token string">""</span> <span class="token punctuation">\</span>      <span class="token parameter variable">-D</span> <span class="token assign-left variable">WITH_CUBLAS</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">ENABLE_FAST_MATH</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">CUDA_FAST_MATH</span><span class="token operator">=</span>ON <span class="token punctuation">\</span>      <span class="token parameter variable">-D</span> <span class="token assign-left variable">ENABLE_NEON</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">WITH_LIBV4L</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">BUILD_TESTS</span><span class="token operator">=</span>OFF <span class="token punctuation">\</span>      <span class="token parameter variable">-D</span> <span class="token assign-left variable">BUILD_PERF_TESTS</span><span class="token operator">=</span>OFF <span class="token parameter variable">-D</span> <span class="token assign-left variable">BUILD_EXAMPLES</span><span class="token operator">=</span>OFF <span class="token punctuation">\</span>      <span class="token parameter variable">-D</span> <span class="token assign-left variable">WITH_QT</span><span class="token operator">=</span>ON <span class="token parameter variable">-D</span> <span class="token assign-left variable">WITH_OPENGL</span><span class="token operator">=</span>ON<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着输入：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">make</span><span class="token function">sudo</span> <span class="token function">make</span> <span class="token function">install</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="7"><li>配置opencv环境：<br><code>sudo gedit /etc/ld.so.conf.d/opencv.conf # 创建并打开该文件，输入以下内容</code><br><img src="/images/Jetson/image-20230322013134812.png" alt="image-20230322013134812"><br>执行下面的命令，使得刚才配置的路径生效<br><code>sudo ldconfig</code><br>然后打开bash.bashrc文件<br><code>sudo gedit /etc/bash.bashrc</code><br>在打开文件的最后加入如下命令</li></ol><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token assign-left variable">PKG_CONFIG_PATH</span><span class="token operator">=</span><span class="token variable">$PKG_CONFIG_PATH</span>:/usr/local/lib/pkgconfig<span class="token builtin class-name">export</span> PKG_CONFIG_PATH<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如图：<br><img src="/images/Jetson/img1.png" alt="img1"><br>然后保存文件，使其生效<br><code>source /etc/bash.bashrc</code><br>8. 检查是否成功安装opencv<br>可以先在jtop查看opencv的安装版本，正确安装后会显示：<br><img src="/images/Jetson/image-20230322013148567.png" alt="image-20230322013148567"><br>9. 之后连接一个摄像头，在终端输入，测试opencv是否能正常使用：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> /opencv-3.4.5/samples/cpp/example_cmakecmake <span class="token builtin class-name">.</span><span class="token function">make</span><span class="token comment"># 成功make后，执行</span>./opencv_example<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="10"><li>上述<code>./opencv_example</code>命令如果失败，原因可能是摄像头的索引错误，<br>输入<code>ls usb</code><br>查看摄像头，发现video1<br>换一个usb接口插入即可，再执行上述命令，可以发现摄像头成功打开。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>成功解决Pytorch模型转trt模型中得BatchNorm问题</title>
      <link href="/2023/05/18/Torch2Trt/"/>
      <url>/2023/05/18/Torch2Trt/</url>
      
        <content type="html"><![CDATA[<p>如果你想把你的模型投入到应用中或者是想提升模型的运行速度，除了对网络进行压缩、蒸馏外，最好的方法就是将模型转成tensor模型，使用tensorrt实现对网络的加速。但是当该模型的功能是图像增强或者是图像生成，并且模型中运用了大量的batchnorm2d函数，运用网上现成的方法会发现模型转成onnx以及trt后，模型的处理效果大幅下降，想解决此问题就可以详细往下看了：<br>我们的方法顺序是：pytorch模型先转成onnx模型，接着将onnx模型转成trt模型</p><h2 id="一、pytorch-to-onnx">一、pytorch to onnx</h2><h3 id="核心代码：">核心代码：</h3><pre name="code" class="python">import torchfrom torchvision.utils import save_imageimport osfrom nets.tiny_unet_2_channelxiangdeng_RCABup3_1_3 import Decoderimport argparseimport torch.nn as nnfrom torchvision import transforms as Tfrom torchvision.datasets import ImageFolderfrom torch.utils import dataimport onnximport onnxruntimeclass Loader(nn.Module):    def __init__(self, image_dir, batch_size=1, num_workers=4):        super(Loader, self).__init__()        self.image_dir = image_dir        self.batch_size = batch_size        self.num_workers = num_workers    def forward(self):        transform = list()        transform.append(T.Resize([704, 1280]))  # 注意要为256的倍数        transform.append(T.ToTensor())        transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))        transform = T.Compose(transform)        dataset = ImageFolder(self.image_dir, transform)        data_loader = data.DataLoader(dataset=dataset,                                      batch_size=self.batch_size,                                      num_workers=self.num_workers)        return data_loaderclass Solver(object):    def __init__(self, data_loader, config):        self.data_loader = data_loader        self.global_g_dim = config.global_G_ngf        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        self.model_save_dir = config.model_save_dir        self.result_dir = config.result_dir        self.build_model()    def restore_model(self, resume_epoch):        """Restore the trained generator and discriminator."""        print('Loading the trained models from step {}...'.format(resume_epoch))        generator_path = os.path.join(self.model_save_dir, '{}-global_G.ckpt'.format(resume_epoch))        self.generator.load_state_dict(torch.load(generator_path, map_location=lambda storage, loc: storage))    def build_model(self):        """create a generator and a discrimin"""        self.generator = Decoder(self.global_g_dim)        self.generator.to(self.device)    def denorm(self, x):        out = (x + 1) / 2        return out.clamp_(0, 1)    def to_numpy(self, tensor):        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()    def test(self):        begin_epoch = 91        end_epoch = 92        step = 1        for k in range(begin_epoch, end_epoch, step):            self.restore_model(94)            self.generator.train()            modelData = "../model/demo" + str(k) + ".onnx"            # modelData = "demo_distok-1ok" + str(k) + ".onnx"            data_loader = self.data_loader            # with torch.no_grad():            for i, (val_img, _) in enumerate(data_loader):  # 出来的图片是RGB格式, 用PIL读取的                if i % 50 != 0:                    continue                x_real_name = data_loader.batch_sampler.sampler.data_source.imgs[i][0]                basename = os.path.basename(x_real_name)                result_path = os.path.join(self.result_dir, str(k))  # 生成的图片的保存文件夹                if not os.path.exists(result_path):                    os.makedirs(result_path)                result_path2 = os.path.join(result_path, basename)                distored_val = val_img.to(self.device)                clean_fake1 = self.generator(distored_val)                torch.cuda.synchronize()                clean_fake1 = clean_fake1.data.cpu()                save_image(self.denorm(clean_fake1), result_path2, nrow=1, padding=0)                torch.onnx.export(self.generator, distored_val, modelData, opset_version=9)                onnx_model = onnx.load(modelData)                onnx.checker.check_model(onnx_model)                ort_session = onnxruntime.InferenceSession(modelData,                                                            providers=['CUDAExecutionProvider'])                ort_inputs = {ort_session.get_inputs()[0].name: self.to_numpy(distored_val)}                ort_outs = ort_session.run(None, ort_inputs)                result_path1 = os.path.join(self.result_dir, "trt"+str(k))                result_pathtrt = os.path.join(result_path1, basename)                #save_image((self.denorm(torch.tensor(ort_outs[0]))),result_pathtrt, nrow=1, padding=0)                result = [val_img, torch.tensor(ort_outs[0]), clean_fake1]                result_concat1 = torch.cat(result, dim=3)                # break                save_image((self.denorm(result_concat1.data.cpu())), result_path2, nrow=2, padding=0)def main(config):    data_loader = Loader(config.data_dir)    solver = Solver(data_loader(), config)    solver.test()    #solver.build_model()    #solver.printnetwork()if __name__ == '__main__':    parser = argparse.ArgumentParser()    parser.add_argument('--global_G_ngf', type=int, default=16, help='number of conv filters in the first layer of G')    parser.add_argument('--num_workers', type=int, default=4)    parser.add_argument('--data_dir', type=str,                        default=r'D:\Data\Paired\underwater_imagenet\val')    parser.add_argument('--model_save_dir', type=str,                        #default='/home/ouc/Li/Torch2TRT/trt_outfile/model'                        default=r'C:\TensorRT-8.6.0.12\samples\python\zzzzFSpiral\modules')    parser.add_argument('--result_dir', type=str,                        default=r'C:\TensorRT-8.6.0.12\samples\python\zzzzFSpiral\result')    torch.cuda.set_device(0)    config = parser.parse_args()    main(config)</pre><p>当pytorch转成onnx时，需要将模型进行eval模式，因为在PyTorch中，模型训练和推理（inference）有两种模式：训练模式和评估模式（evaluation mode）。在训练模式下，模型会保留一些中间结果，以便进行反向传播和参数更新(如batchnorm2d在train和eval模型中的计算不一致，涉及到内部的变量running_mean和running_eval的计算（如下图所示）)。而在评估模式下，模型不会保留这些中间结果，以便进行推理。因此，在将PyTorch模型转换为ONNX模型时，需要将模型切换到评估模式，以确保模型的输出与推理结果一致。如果在转换模型之前不将模型切换到评估模式，可能会导致转换后的ONNX模型输出与预期不一致。因此，为了确保转换后的ONNX模型的正确性，需要将PyTorch模型切换到评估模式（eval mode）再进行转换。</p><pre name="code" class="python">class MyBatchNorm2d(nn.BatchNorm2d):    def __init__(self, num_features, eps=1e-5, momentum=0.1,                 affine=True, track_running_stats=True):        super(MyBatchNorm2d, self).__init__(            num_features, eps, momentum, affine, track_running_stats)    def forward(self, input):        self._check_input_dim(input)        exponential_average_factor = 0.0        if self.training and self.track_running_stats:            if self.num_batches_tracked is not None:                self.num_batches_tracked += 1                if self.momentum is None:  # use cumulative moving average                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)                else:  # use exponential moving average                    exponential_average_factor = self.momentum        # calculate running estimates        if self.training:            mean = input.mean([0, 2, 3])            # use biased var in train            var = input.var([0, 2, 3], unbiased=False)            n = input.numel() / input.size(1)            with torch.no_grad():                self.running_mean = exponential_average_factor * mean\                    + (1 - exponential_average_factor) * self.running_mean                # update running_var with unbiased var                self.running_var = exponential_average_factor * var * n / (n - 1)\                    + (1 - exponential_average_factor) * self.running_var        else:            mean = self.running_mean            var = self.running_var        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))        if self.affine:            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]        return input</pre><p>当模型是train时，其<code>mean = input.mean([0, 2, 3])；var = input.var([0, 2, 3], unbiased=False)</code>，计算与当前的输入相挂钩，但是当模型是eval模式时，其<code>mean = self.running_mean；var = self.running_var</code>，计算与当前的输入无关，而是将学习到的running_mean与running_var直接作为当前计算的mean与var，由于该学习得到的均值与方差（running_mean与running_var）并不能完全适用所有的输入样例，因此模型在eval模式下的处理效果极大降低。</p><p>但是有人可能会想为什么模型转成onnx时要将模型设置为eval模式，直接设置为train模式不就可以解决处理效果下降的问题了吗？但是当你按该想法实验的时候，会发现虽然pytorch模型的处理效果没问题，但是onnx模型的处理效果还是一样的差，这又是为什么呢？我的猜测是onnx模型在推理时默认将batchnorm的模式设置为eval，进而输出的结果就存在问题了。这个是我个人的想法，大家有不同意见可以互相交流。</p><p>所以现在的问题就转换到了如何使模型在eval模式下计算的mean与var和当前输入相关，而不是依赖学习到的running_mean与running_eval。部分人可能遇到此问题就无从下手了，认为该问题是无法解决的。但是这个问题在我这里是能解决的。首先经过上述对问题的描述，大家对这个问题已经是比较清楚了。</p><p>接下来我将介绍两种很简单的方法来解决onnx模型处理效果差的问题：</p><h3 id="方法一：">方法一：</h3><h4 id="具体措施">具体措施:</h4><p>修改batchnorm2d中的源码，在if self.training：上方加一行self.training =True。这样尽管处于eval模式，但是batchnorm仍然是处于训练状态的，在这种情况下，mean与var自然就和input挂钩了。<br><img src="/images/Pytorch2trt/image-2.png" style="zoom: 200%;" /></p><h4 id="利弊：">利弊：</h4><p>不过这种方法虽然可以解决onnx模型处理效果差的问题，但是由该方法获得的onnx模型是无法成功转tensorrt的。原因是因为onnx模型的参数都是固定死的，正如上述我所展示的代码中，当模型处于训练模式时，running_mean与running_var是变化的，因此获得的onnx尽管输出没问题，但其内部是存在问题的。<br>batchnorm2d的源代码路径如下:<br><code>C:\Users\Spring\Anaconda3\envs\torchli\Lib\site-packages\torch\nn\modules\batchnorm.py</code></p><h3 id="方法二">方法二</h3><h4 id="具体措施-2">具体措施:</h4><p>修改batchnorm2d的源码，不过与方法一有所区别，具体修改见下图，之所以这样修改是因为batchnorm在eval模式下，mean与var和running_mean与running_val挂钩，故我们提前将学习到的running_mean与running_var设置成与input挂钩，然后就mean与var就和input联系起来了。<br><img src="/images/Pytorch2trt/image-20230514162311051.png" style="zoom: 200%;" /></p><h4 id="利弊：-2">利弊：</h4><p>不过这种方法虽然可以解决onnx模型处理效果差的问题，但是当可视化由该方法获得的onnx模型时，会发现batchnorm是不纯净的，上方多了一些计算分支（如右图所示），但是方法一的batchnorm是纯净的（如左图所示）。好处是由该方法获得的onnx模型是可以顺利转成trt模型的。<br><img src="/images/Pytorch2trt/image-1.png" style="zoom:150%;" /></p><h2 id="二、onnx-to-trt">二、onnx to trt</h2><h3 id="终端转换命令：">终端转换命令：</h3><p>Windows<br><code>.\trtexec.exe --onnx=aaa.onnx --saveEngine=retinate_hat_hair_beard_sim.trt --device=0</code><br>Ubuntu<br><code>trtexec --onnx=aaa.onnx --saveEngine=retinate_hat_hair_beard_sim.trt --device=0</code></p><h3 id="使用trt模型进行推理：">使用trt模型进行推理：</h3><pre name="code" class="python">import torchfrom torch.autograd import Variableimport tensorrt as trtimport pycuda.driver as cudafrom torchvision.utils import save_imageimport osimport pycuda.gpuarray as gpuarrayimport pycuda.autoinitfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoaderfrom torchvision import transforms as Tfrom torchvision.datasets import ImageFolderimport timeimport numpy as npfrom torch.utils import dataimport torch.nn as nnimport time# 操作缓存，进行运算， 这个函数是通用的def infer(context, input_img, output_size, batch_size=1):    # Convert input data to Float32,这个类型要转换，不严会有好多报错    input_img = input_img.astype(np.float32)    # Create output array to receive data    output = np.empty(output_size, dtype=np.float32)    # output = np.empty(batch_size * output.nbytes)    # Allocate device memory    d_input = cuda.mem_alloc(batch_size * input_img.nbytes)    d_output = cuda.mem_alloc(batch_size * output.nbytes)    bindings = [int(d_input), int(d_output)]    stream = cuda.Stream()    # Transfer input data to device    cuda.memcpy_htod_async(d_input, input_img, stream)    # Execute model    context.execute_async(batch_size, bindings, stream.handle, None)    # Transfer predictions back    cuda.memcpy_dtoh_async(output, d_output, stream)    stream.synchronize()    # Return predictions    return outputclass Loader(nn.Module):    def __init__(self, image_dir, batch_size=1, num_workers=4):        super(Loader, self).__init__()        self.image_dir = image_dir        self.batch_size = batch_size        self.num_workers = num_workers    def forward(self):        transform = list()        transform.append(T.Resize([704, 1280]))  # 注意要为256的倍数        transform.append(T.ToTensor())        transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))        transform = T.Compose(transform)        dataset = ImageFolder(self.image_dir, transform)        data_loader = data.DataLoader(dataset=dataset,                                      batch_size=self.batch_size,                                      num_workers=self.num_workers)        return data_loaderdef denorm(x):    out = (x + 1) / 2    return out.clamp_(0, 1)# 执行测试函数def do_test(context, batch_size=1):    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    test_loader = Loader(image_dir='/home/ouc/LiModel/data-paired/underwater_imagenet/val/')    print("mnist data load successful!!!")    accurary = 0    start_time = time.time()    times = []    for i, (val_img, _) in enumerate(test_loader()):    # 开始测试        # img, label = data        x_real_name = test_loader().batch_sampler.sampler.data_source.imgs[i][0]        basename = os.path.basename(x_real_name)        img = val_img.numpy()    # 这个数据要从torch.Tensor转换成numpy格式的        # print(img)        # label = Variable(label, volatile=True)        # with torch.no_grad():        #     label = Variable(label)        start_time1 = time.time()        output = infer(context, img, img.shape, 1)        end_time1 = time.time()        print(end_time1 - start_time1)        times.append((end_time1 - start_time1))        result_trt = torch.tensor(output)        # print(result_trt.shape)        # print(val_img.shape)        result = [val_img, result_trt]        result_concat1 = torch.cat(result, dim=3)        save_image((denorm(result_concat1.data.cpu())), '/home/ouc/Desktop/python/zzzzFSpiral/result/trt/' + basename, nrow=2, padding=0)        #print(output)        # conf, pred = torch.max(torch.Tensor(output), -1)        # num_count = (pred == label).sum()        # accurary += num_count.data    end_time = time.time()    print((start_time - end_time)/len(test_loader()))    print(np.mean(times))    # print("Test Acc is {:.6f}".format(accurary / len(test_dataset())))    # return accurary/len(test_dataset), time.time() - start_timedef trt_infer():    # 读取.trt文件    def loadEngine2TensorRT(filepath):        G_LOGGER = trt.Logger(trt.Logger.WARNING)        # 反序列化引擎        with open(filepath, "rb") as f, trt.Runtime(G_LOGGER) as runtime:            engine = runtime.deserialize_cuda_engine(f.read())            return engine    trt_model_name = "./resnet3.trt"    engine = loadEngine2TensorRT(trt_model_name)    # 创建上下文    context = engine.create_execution_context()    print("Start TensorRT Test...")    do_test(context)    # print('INT8 acc: {}, need time: {}'.format(acc, times))if __name__ == '__main__':    trt_infer()</pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/05/17/hello-world/"/>
      <url>/2023/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
